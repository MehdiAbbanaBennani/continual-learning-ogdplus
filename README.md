# Continual Learning with OGD and OGD+

This is the official implementation of the article [Generalisation Guarantees for Continual Learning with Orthogonal
 Gradient
 Descent](https://arxiv.org/abs/2006.11942) in PyTorch.
 
## Requirements
 PyTorch >= 1.5.0 

## Reproducibility
In order to replicate the results of the paper, please run the scripts provided in the scripts directory.
 
## Questions/ Bugs
- For questions or bugs, please feel free to contact Mehdi Abbana Bennani or to raise an issue on Github :)

## Citation
If this repository helps your work, please cite:

```
@article{bennani2020generalisation,
    title={Generalisation Guarantees for Continual Learning with Orthogonal Gradient Descent},
    author={Mehdi Abbana Bennani and Masashi Sugiyama},
    year={2020},
    journal={ICML 4th Lifelong Learning Workshop},
}
```


## Licence
A substantial part of this source code was initially forked from the repository [GT-RIPL/Continual-Learning-Benchmark
](https://github.com/GT-RIPL/Continual-Learning-Benchmark). The associated Licence is also provided in the root
 directory.  
 The work related to the original source code is the following : 
 ```
@inproceedings{Hsu18_EvalCL,
  title={Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines},
  author={Yen-Chang Hsu and Yen-Cheng Liu and Anita Ramasamy and Zsolt Kira},
  booktitle={NeurIPS Continual learning Workshop },
  year={2018},
  url={https://arxiv.org/abs/1810.12488}
}
```
 
 
 
 This source code is released under The MIT License found in the LICENSE file in the root directory of this source tree. 